


!git clone https://github.com/lijingwang/hbvpy.git
%cd hbvpy





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import qmc
from pathlib import Path
import warnings
warnings.filterwarnings("ignore")
%config InlineBackend.figure_format = 'retina'

from hbv import hbv_run





example_dir = Path("examples") / "Prof_Amir_AghaKouchak_example"

forcing = pd.read_csv(example_dir / "inputPrecipTemp.csv", parse_dates=["Time"])
forcing["Time"] = pd.to_datetime(forcing["Time"], format="%Y-%m-%d")
pet = pd.read_csv(example_dir / "inputMonthlyTempEvap.csv")
Qobs = pd.read_csv(example_dir / "Qobs.csv", parse_dates=["Time"])
Qobs["Time"] = pd.to_datetime(Qobs["Time"], format="%Y-%m-%d")

print(f"Simulation period: {forcing['Time'].min()} to {forcing['Time'].max()}")
print(f"Number of days: {len(forcing)}")





# Parameter names and their distributions (min, max for uniform)
param_names = ["d", "fc", "beta", "cpar", "k0", "lthr", "k1", "k2", "kp", "pwp"]

param_bounds = {
    "d":    (1.0, 5.0),      # degree-day melt factor (mm/degC/day)
    "fc":   (100.0, 400.0),  # field capacity (mm)
    "beta": (1.0, 5.0),      # runoff nonlinearity exponent
    "cpar": (0.01, 0.3),     # PET temperature correction factor
    "k0":   (0.1, 0.5),      # quickflow coefficient (1/day)
    "lthr": (1.0, 20.0),     # threshold in upper storage (mm)
    "k1":   (0.01, 0.2),     # interflow coefficient (1/day)
    "k2":   (0.005, 0.15),   # baseflow coefficient (1/day)
    "kp":   (0.01, 0.15),    # percolation coefficient (1/day)
    "pwp":  (50.0, 200.0),   # permanent wilting point (mm)
}

# Extract lower and upper bounds as arrays
lower_bounds = np.array([param_bounds[p][0] for p in param_names])
upper_bounds = np.array([param_bounds[p][1] for p in param_names])

n_params = len(param_names)
print(f"Number of parameters: {n_params}")





def monte_carlo_sampling(n_samples, lower, upper, seed=42):
    """
    Generate parameter samples using Monte Carlo (random) sampling.
    
    Parameters
    ----------
    n_samples : int
        Number of samples to generate
    lower : array-like
        Lower bounds for each parameter
    upper : array-like
        Upper bounds for each parameter
    seed : int
        Random seed for reproducibility
    
    Returns
    -------
    samples : ndarray, shape (n_samples, n_params)
        Parameter samples
    """
    np.random.seed(seed)
    n_params = len(lower)
    samples = np.zeros((n_samples, n_params))
    
    for i in range(n_params):
        samples[:, i] = np.random.uniform(lower[i], upper[i], n_samples)
    
    return samples


def latin_hypercube_sampling(n_samples, lower, upper, seed=42):
    """
    Generate parameter samples using Latin Hypercube Sampling (LHS).
    
    Parameters
    ----------
    n_samples : int
        Number of samples to generate
    lower : array-like
        Lower bounds for each parameter
    upper : array-like
        Upper bounds for each parameter
    seed : int
        Random seed for reproducibility
    
    Returns
    -------
    samples : ndarray, shape (n_samples, n_params)
        Parameter samples
    """
    n_params = len(lower)
    
    # Create LHS sampler using scipy.stats.qmc
    sampler = qmc.LatinHypercube(d=n_params, seed=seed)
    
    # Generate samples in [0, 1] hypercube
    unit_samples = sampler.random(n=n_samples)
    
    # Scale to parameter bounds
    samples = qmc.scale(unit_samples, lower, upper)
    
    return samples


# Number of samples for uncertainty propagation
N_SAMPLES = 500

# Generate samples using both methods
mc_samples = monte_carlo_sampling(N_SAMPLES, lower_bounds, upper_bounds, seed=42)
lhs_samples = latin_hypercube_sampling(N_SAMPLES, lower_bounds, upper_bounds, seed=42)

print(f"Monte Carlo samples shape: {mc_samples.shape}")
print(f"Latin Hypercube samples shape: {lhs_samples.shape}")





fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Plot first two parameters
p1, p2 = 0, 1  # d and fc

n = 500
axes[0].scatter(mc_samples[:n, p1], mc_samples[:n, p2], alpha=0.5, s=20)
axes[0].set_xlabel(f"{param_names[p1]}")
axes[0].set_ylabel(f"{param_names[p2]}")
axes[0].set_title(f"Monte Carlo Sampling (N={N_SAMPLES})")
axes[0].set_xlim(lower_bounds[p1], upper_bounds[p1])
axes[0].set_ylim(lower_bounds[p2], upper_bounds[p2])

axes[1].scatter(lhs_samples[:n, p1], lhs_samples[:n, p2], alpha=0.5, s=20, c='C1')
axes[1].set_xlabel(f"{param_names[p1]}")
axes[1].set_ylabel(f"{param_names[p2]}")
axes[1].set_title(f"Latin Hypercube Sampling (N={N_SAMPLES})")
axes[1].set_xlim(lower_bounds[p1], upper_bounds[p1])
axes[1].set_ylim(lower_bounds[p2], upper_bounds[p2])

plt.tight_layout()
plt.show()





def run_ensemble(samples, forcing, pet, area_km2=410.0, verbose=True):
    """
    Run HBV model for an ensemble of parameter samples.
    
    Parameters
    ----------
    samples : ndarray, shape (n_samples, n_params)
        Parameter samples
    forcing : DataFrame
        Forcing data (precipitation, temperature)
    pet : DataFrame
        Monthly PET data
    area_km2 : float
        Catchment area in km^2
    verbose : bool
        Print progress updates
    
    Returns
    -------
    Q_ensemble : ndarray, shape (n_samples, n_days)
        Discharge time series for each parameter set
    time : array
        Time array
    """
    n_samples = samples.shape[0]
    n_days = len(forcing)
    Q_ensemble = np.zeros((n_samples, n_days))
    
    for i in range(n_samples):
        if verbose and (i + 1) % 100 == 0:
            print(f"  Running simulation {i+1}/{n_samples}")
        
        params = samples[i, :]
        results, _ = hbv_run(
            forcing=forcing,
            pet_monthly=pet,
            params=params,
            area_km2=area_km2,
            Tsnow_thresh=0.0,
            init_state={"snow": 0.0, "soil": 0.0, "s1": 0.0, "s2": 0.0},
        )
        Q_ensemble[i, :] = results["Q_m3s"].values
    
    time = results["Time"].values
    return Q_ensemble, time


print("Running Monte Carlo ensemble...")
Q_mc, time = run_ensemble(mc_samples, forcing, pet)
time_index = pd.DatetimeIndex(time)

print("\nRunning Latin Hypercube ensemble...")
Q_lhs, _ = run_ensemble(lhs_samples, forcing, pet)

print(f"\nEnsemble output shape: {Q_mc.shape} (samples x days)")





def calculate_prediction_intervals(Q_ensemble, percentiles=[2.5, 50, 97.5]):
    """
    Calculate prediction intervals from ensemble.
    
    Parameters
    ----------
    Q_ensemble : ndarray, shape (n_samples, n_days)
        Ensemble of discharge time series
    percentiles : list
        Percentiles to calculate (default: 2.5, 50, 97.5 for 95% CI)
    
    Returns
    -------
    results : dict
        Dictionary with percentile arrays
    """
    results = {}
    for p in percentiles:
        results[f"p{p}"] = np.percentile(Q_ensemble, p, axis=0)
    results["mean"] = np.mean(Q_ensemble, axis=0)
    results["std"] = np.std(Q_ensemble, axis=0)
    return results


# Calculate prediction intervals for both methods
mc_stats = calculate_prediction_intervals(Q_mc)
lhs_stats = calculate_prediction_intervals(Q_lhs)

print("Statistics calculated for both ensembles.")


def report_pi(date_str, label, stats):
    idx = time_index.get_loc(pd.Timestamp(date_str))
    lo  = stats["p2.5"][idx]
    hi  = stats["p97.5"][idx]
    med = stats["p50"][idx]
    print(f"{label} ({date_str})")
    print(f"  2.5th percentile : {lo:.3f} m³/s")
    print(f"  Median           : {med:.3f} m³/s")
    print(f"  97.5th percentile: {hi:.3f} m³/s")
    print(f"  95% PI width     : {hi - lo:.3f} m³/s")
    print()

print("LHS ensemble (N=500) — 95% Prediction Intervals")
print("=" * 50)
DATE_  = "1994-10-01"
report_pi(DATE_, "Baseflow date", lhs_stats)





fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)

# Monte Carlo results
ax = axes[0]
ax.fill_between(time, mc_stats["p2.5"], mc_stats["p97.5"], 
                alpha=0.3, color='C0', label='95% Prediction Interval')
ax.plot(time, mc_stats["p50"], 'C0-', lw=1.5, label='Median')
ax.plot(Qobs["Time"], Qobs["Q (m3/s)"], 'k-', lw=1, alpha=0.7, label='Observed')
ax.set_ylabel("Discharge Q (m$^3$/s)")
ax.set_title(f"Monte Carlo Sampling (N={N_SAMPLES})")
ax.legend(loc='upper right')
ax.set_ylim(bottom=0)

# Latin Hypercube results
ax = axes[1]
ax.fill_between(time, lhs_stats["p2.5"], lhs_stats["p97.5"], 
                alpha=0.3, color='C1', label='95% Prediction Interval')
ax.plot(time, lhs_stats["p50"], 'C1-', lw=1.5, label='Median')
ax.plot(Qobs["Time"], Qobs["Q (m3/s)"], 'k-', lw=1, alpha=0.7, label='Observed')
ax.set_xlabel("Time")
ax.set_ylabel("Discharge Q (m$^3$/s)")
ax.set_title(f"Latin Hypercube Sampling (N={N_SAMPLES})")
ax.legend(loc='upper right')
ax.set_ylim(bottom=0)

plt.tight_layout()
plt.show()





# Calculate interval width (uncertainty)
mc_interval_width = mc_stats["p97.5"] - mc_stats["p2.5"]
lhs_interval_width = lhs_stats["p97.5"] - lhs_stats["p2.5"]

print("=" * 60)
print("UNCERTAINTY PROPAGATION SUMMARY (N=500)")
print("=" * 60)
print(f"Number of samples: {N_SAMPLES}")
print(f"Number of parameters: {n_params}")
print(f"Simulation period: {len(time)} days")
print()
print("Monte Carlo Sampling:")
print(f"  Mean 95% PI width: {np.mean(mc_interval_width):.2f} m3/s")
print(f"  Max 95% PI width:  {np.max(mc_interval_width):.2f} m3/s")
print(f"  Mean discharge:    {np.mean(mc_stats['mean']):.2f} m3/s")
print()
print("Latin Hypercube Sampling:")
print(f"  Mean 95% PI width: {np.mean(lhs_interval_width):.2f} m3/s")
print(f"  Max 95% PI width:  {np.max(lhs_interval_width):.2f} m3/s")
print(f"  Mean discharge:    {np.mean(lhs_stats['mean']):.2f} m3/s")
print("=" * 60)





# Sample sizes to compare (using subsets of existing 500 samples)
sample_sizes = [500, 100, 50, 20, 10]

# Store results for each sample size
results_by_size = {}

for n in sample_sizes:
    # Use first n samples from the already computed ensembles
    Q_mc_n = Q_mc[:n, :]
    Q_lhs_n = Q_lhs[:n, :]
    
    # Calculate statistics
    mc_stats_n = calculate_prediction_intervals(Q_mc_n)
    lhs_stats_n = calculate_prediction_intervals(Q_lhs_n)
    
    # Store results
    results_by_size[n] = {
        'mc': {
            'stats': mc_stats_n,
            'mean_pi_width': np.mean(mc_stats_n["p97.5"] - mc_stats_n["p2.5"]),
            'max_pi_width': np.max(mc_stats_n["p97.5"] - mc_stats_n["p2.5"]),
            'mean_discharge': np.mean(mc_stats_n['mean']),
        },
        'lhs': {
            'stats': lhs_stats_n,
            'mean_pi_width': np.mean(lhs_stats_n["p97.5"] - lhs_stats_n["p2.5"]),
            'max_pi_width': np.max(lhs_stats_n["p97.5"] - lhs_stats_n["p2.5"]),
            'mean_discharge': np.mean(lhs_stats_n['mean']),
        }
    }

print("Statistics calculated for sample sizes:", sample_sizes)





# Create summary table
print("\n" + "="*90)
print("SUMMARY STATISTICS BY SAMPLE SIZE")
print("="*90)
print(f"{'N':>6} | {'Method':>6} | {'Mean 95% PI Width':>18} | {'Max 95% PI Width':>17} | {'Mean Q':>10}")
print("-"*90)

for n in sample_sizes:
    mc_res = results_by_size[n]['mc']
    lhs_res = results_by_size[n]['lhs']
    
    print(f"{n:>6} | {'MC':>6} | {mc_res['mean_pi_width']:>15.2f} m3/s | {mc_res['max_pi_width']:>14.2f} m3/s | {mc_res['mean_discharge']:>7.2f} m3/s")
    print(f"{n:>6} | {'LHS':>6} | {lhs_res['mean_pi_width']:>15.2f} m3/s | {lhs_res['max_pi_width']:>14.2f} m3/s | {lhs_res['mean_discharge']:>7.2f} m3/s")
    print("-"*90)

print("="*90)





fig, axes = plt.subplots(1, 1, figsize=(7, 5))

# Extract data for plotting
mc_mean_pi = [results_by_size[n]['mc']['mean_pi_width'] for n in sample_sizes]
lhs_mean_pi = [results_by_size[n]['lhs']['mean_pi_width'] for n in sample_sizes]
mc_max_pi = [results_by_size[n]['mc']['max_pi_width'] for n in sample_sizes]
lhs_max_pi = [results_by_size[n]['lhs']['max_pi_width'] for n in sample_sizes]

# Plot Mean 95% PI Width
ax = axes
ax.plot(sample_sizes, mc_mean_pi, 'o-', color='C0', lw=2, markersize=8, label='Monte Carlo')
ax.plot(sample_sizes, lhs_mean_pi, 's--', color='C1', lw=2, markersize=8, label='Latin Hypercube')
ax.set_xlabel('Number of Samples (N)')
ax.set_ylabel('Mean 95% PI Width (m$^3$/s)')
ax.set_title('Mean Prediction Interval Width vs Sample Size')
ax.legend()
ax.set_xscale('log')
ax.set_xticks(sample_sizes)
ax.set_xticklabels(sample_sizes)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()





import matplotlib.dates as mdates

# ── Align observed Q to the model time index ─────────────────────────────────
obs_aligned = Qobs.set_index("Time").reindex(time_index)["Q (m3/s)"].values

lo_arr  = lhs_stats["p2.5"]
hi_arr  = lhs_stats["p97.5"]
med_arr = lhs_stats["p50"]

# Boolean mask: obs outside 95% PI
outside = (obs_aligned < lo_arr) | (obs_aligned > hi_arr)

# ── Helper to annotate one date on an axes ───────────────────────────────────
def mark_date(ax, date_str, lo_arr, hi_arr, med_arr, color, label):
    idx = time_index.get_loc(pd.Timestamp(date_str))
    t   = time_index[idx]
    ax.axvline(t, color=color, lw=1.2, ls="--", alpha=0.8)
    ax.annotate(
        f"{label}\n[{lo_arr[idx]:.2f}, {hi_arr[idx]:.2f}]",
        xy=(t, hi_arr[idx]),
        xytext=(10, 6), textcoords="offset points",
        fontsize=7.5, color=color,
        arrowprops=dict(arrowstyle="-", color=color, lw=0.8),
    )

# ── Figure: full period (top) + WY zoom-in (bottom) ─────────────────────────
fig, axes = plt.subplots(2, 1, figsize=(15, 10),
                         gridspec_kw={"height_ratios": [2, 1.4]})

for ax, zoom in zip(axes, [False, True]):
    # Shaded 95% PI
    ax.fill_between(time_index, lo_arr, hi_arr,
                    alpha=0.35, color="C1", label="95% PI (LHS)")
    # Median
    ax.plot(time_index, med_arr, color="C1", lw=1.2, label="Median (LHS)")
    # Observed — inside interval
    ax.plot(time_index[~outside], obs_aligned[~outside],
            "k-", lw=0.8, label="Observed (inside PI)")
    # Observed — outside interval
    ax.scatter(time_index[outside], obs_aligned[outside],
               color="red", s=12, zorder=5, label="Observed (outside PI)")

    # Mark the two target dates
    mark_date(ax, DATE_BASEFLOW, lo_arr, hi_arr, med_arr, "royalblue",  "Baseflow")
    mark_date(ax, DATE_PEAKFLOW, lo_arr, hi_arr, med_arr, "darkorange", "Peak flow")

    ax.set_ylabel("Discharge (m³/s)")
    ax.set_ylim(bottom=0)
    ax.grid(True, alpha=0.25)
    ax.legend(loc="upper right", fontsize=8)

    if zoom:
        ax.set_xlim(pd.Timestamp(WY_START), pd.Timestamp(WY_END))
        ax.set_title(f"Zoom-in: WY {pd.Timestamp(WY_END).year}  "
                     f"({WY_START} – {WY_END})")
        ax.xaxis.set_major_formatter(mdates.DateFormatter("%b %Y"))
        ax.xaxis.set_major_locator(mdates.MonthLocator())
        plt.setp(ax.get_xticklabels(), rotation=30, ha="right")
        ax.set_xlabel("Date")
    else:
        ax.set_title(f"Full simulation period — LHS 95% Prediction Interval (N=500)\n"
                     f"Red dots = observations outside 95% PI")
        ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y"))
        ax.xaxis.set_major_locator(mdates.YearLocator())

fig.tight_layout()
plt.show()

n_outside = int(np.sum(outside & ~np.isnan(obs_aligned)))
n_total   = int(np.sum(~np.isnan(obs_aligned)))
print(f"Observations outside 95% PI: {n_outside} / {n_total} days  "
      f"({100*n_outside/n_total:.1f}%)")
