


!git clone https://github.com/lijingwang/hbvpy.git
%cd hbvpy


!pip install SALib pyDGSA -q





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import qmc
from pathlib import Path
import warnings
warnings.filterwarnings("ignore")
%config InlineBackend.figure_format = 'retina'

from hbv import hbv_run





example_dir = Path("examples") / "Prof_Amir_AghaKouchak_example"

forcing = pd.read_csv(example_dir / "inputPrecipTemp.csv", parse_dates=["Time"])
forcing["Time"] = pd.to_datetime(forcing["Time"], format="%Y-%m-%d")
pet = pd.read_csv(example_dir / "inputMonthlyTempEvap.csv")
Qobs = pd.read_csv(example_dir / "Qobs.csv", parse_dates=["Time"])
Qobs["Time"] = pd.to_datetime(Qobs["Time"], format="%Y-%m-%d")

param_names = ["d", "fc", "beta", "cpar", "k0", "lthr", "k1", "k2", "kp", "pwp"]

param_bounds = {
    "d":    (1.0, 5.0),
    "fc":   (100.0, 400.0),
    "beta": (1.0, 5.0),
    "cpar": (0.01, 0.3),
    "k0":   (0.1, 0.5),
    "lthr": (1.0, 20.0),
    "k1":   (0.01, 0.2),
    "k2":   (0.005, 0.15),
    "kp":   (0.01, 0.15),
    "pwp":  (50.0, 200.0),
}

lower_bounds = np.array([param_bounds[p][0] for p in param_names])
upper_bounds = np.array([param_bounds[p][1] for p in param_names])
n_params = len(param_names)

print(f"Simulation period: {forcing['Time'].min()} to {forcing['Time'].max()}")
print(f"Number of days: {len(forcing)}")
print(f"Number of parameters: {n_params}")


WARMUP = 365  # skip first year

def hbv_run_ts(params):
    """Run HBV model and return the full discharge time series."""
    results, _ = hbv_run(
        forcing=forcing, pet_monthly=pet, params=params,
        area_km2=410.0, Tsnow_thresh=0.0,
        init_state={"snow": 0.0, "soil": 0.0, "s1": 0.0, "s2": 0.0},
    )
    return results["Q_m3s"].values

def hbv_metrics(params):
    """Run HBV and return dict of {mean_Q, Q10, Q90} after warmup.
    Q10/Q90 use the exceedance probability convention:
      Q10 = flow exceeded 10% of the time (high flow)
      Q90 = flow exceeded 90% of the time (low flow)
    """
    Q = hbv_run_ts(params)[WARMUP:]
    return {
        "mean_Q": np.mean(Q),
        "Q10":    np.percentile(Q, 90),   # exceeded 10% of time = high flow
        "Q90":    np.percentile(Q, 10),   # exceeded 90% of time = low flow
    }

metric_labels = {"mean_Q": "Mean Q", "Q10": "Q10 (High Flow)", "Q90": "Q90 (Low Flow)"}

# Baseline
baseline_params = (lower_bounds + upper_bounds) / 2
baseline_metrics = hbv_metrics(baseline_params)
print(f"Baseline: {baseline_metrics}")





delta_frac = 0.05
metrics_list = ["mean_Q", "Q10", "Q90"]
oat_elasticities = {m: np.zeros(n_params) for m in metrics_list}

for i in range(n_params):
    delta = delta_frac * (upper_bounds[i] - lower_bounds[i])

    params_plus = baseline_params.copy()
    params_plus[i] += delta
    m_plus = hbv_metrics(params_plus)

    params_minus = baseline_params.copy()
    params_minus[i] -= delta
    m_minus = hbv_metrics(params_minus)

    for m in metrics_list:
        sens = (m_plus[m] - m_minus[m]) / (2 * delta)
        oat_elasticities[m][i] = sens * baseline_params[i] / baseline_metrics[m]

print(f"OAT: {2*n_params + 1} model evaluations (1 baseline + 2x{n_params} perturbations)")


fig, axes = plt.subplots(1, 3, figsize=(16, 5), sharey=False)

for ax, m in zip(axes, metrics_list):
    sorted_idx = np.argsort(np.abs(oat_elasticities[m]))[::-1]  # rank 1 at top
    sorted_names = [param_names[i] for i in sorted_idx]
    sorted_elast = oat_elasticities[m][sorted_idx]
    colors = ["C0" if e >= 0 else "C3" for e in sorted_elast]
    y_pos = np.arange(n_params, 0, -1)  # top-down: rank 1 at top
    ax.barh(y_pos, sorted_elast, color=colors, edgecolor="k", linewidth=0.5)
    ax.set_yticks(y_pos)
    ax.set_yticklabels(sorted_names)
    ax.set_xlabel("Elasticity")
    ax.set_title(f"OAT: {metric_labels[m]}")
    ax.axvline(x=0, color="k", linewidth=0.8)

plt.suptitle("OAT Local Sensitivity: Tornado Diagrams", fontsize=14, y=1.02)
plt.tight_layout()
plt.show()





from SALib.sample.morris import sample as morris_sample
from SALib.analyze.morris import analyze as morris_analyze

problem = {
    'num_vars': n_params,
    'names': param_names,
    'bounds': [[lower_bounds[i], upper_bounds[i]] for i in range(n_params)]
}

X_morris = morris_sample(problem, N=100, num_levels=4, seed=42)
print(f"Morris samples: {X_morris.shape[0]} model evaluations")

# Run model and collect all three metrics
Y_morris = {m: np.zeros(X_morris.shape[0]) for m in metrics_list}

for i in range(X_morris.shape[0]):
    Q = hbv_run_ts(X_morris[i])
    Q_post = Q[WARMUP:]
    Y_morris["mean_Q"][i] = np.mean(Q_post)
    Y_morris["Q10"][i] = np.percentile(Q_post, 90)   # exceeded 10% of time
    Y_morris["Q90"][i] = np.percentile(Q_post, 10)   # exceeded 90% of time
    if (i + 1) % 200 == 0:
        print(f"  Completed {i+1}/{X_morris.shape[0]}")

# Analyze for each metric
Si_morris = {}
for m in metrics_list:
    Si_morris[m] = morris_analyze(problem, X_morris, Y_morris[m])
    print(f"\n{metric_labels[m]}:")
    for j in range(n_params):
        print(f"  {param_names[j]:<8} mu*={Si_morris[m]['mu_star'][j]:>8.4f}  sigma={Si_morris[m]['sigma'][j]:>8.4f}")


fig, axes = plt.subplots(1, 3, figsize=(15, 5))

for ax, m in zip(axes, metrics_list):
    si = Si_morris[m]
    ax.scatter(si['mu_star'], si['sigma'], s=100, c='C0', edgecolors='k', zorder=5)
    for j in range(n_params):
        ax.annotate(param_names[j],
                    (si['mu_star'][j], si['sigma'][j]),
                    textcoords='offset points', xytext=(8, 5), fontsize=10)
    max_val = max(max(si['mu_star']), max(si['sigma'])) * 1.1
    ax.plot([0, max_val], [0, max_val], 'k--', alpha=0.3)
    ax.set_xlabel(r'$\mu^*$', fontsize=12)
    ax.set_ylabel(r'$\sigma$', fontsize=12)
    ax.set_title(f'Morris: {metric_labels[m]}')

plt.suptitle(r'Morris Method: $\mu^*$ vs $\sigma$', fontsize=14, y=1.02)
plt.tight_layout()
plt.show()





from SALib.sample import saltelli
from SALib.analyze import sobol

# Generate Saltelli samples with second-order enabled
X_sobol = saltelli.sample(problem, N=256, calc_second_order=True)
print(f"Sobol samples: {X_sobol.shape[0]} model evaluations")

# Run model once, compute all three metrics per evaluation
Y_sobol = {m: np.zeros(X_sobol.shape[0]) for m in metrics_list}

for i in range(X_sobol.shape[0]):
    Q = hbv_run_ts(X_sobol[i])
    Q_post = Q[WARMUP:]
    Y_sobol["mean_Q"][i] = np.mean(Q_post)
    Y_sobol["Q10"][i] = np.percentile(Q_post, 90)   # exceeded 10% of time
    Y_sobol["Q90"][i] = np.percentile(Q_post, 10)   # exceeded 90% of time
    if (i + 1) % 1000 == 0:
        print(f"  Completed {i+1}/{X_sobol.shape[0]}")

# Analyze for each metric
Si_sobol = {}
for m in metrics_list:
    Si_sobol[m] = sobol.analyze(problem, Y_sobol[m], calc_second_order=True)

for m in metrics_list:
    print(f"\n{metric_labels[m]}:")
    for j in range(n_params):
        print(f"  {param_names[j]:<8} S1={Si_sobol[m]['S1'][j]:>8.4f}  ST={Si_sobol[m]['ST'][j]:>8.4f}")


fig, axes = plt.subplots(1, 3, figsize=(18, 5))
x = np.arange(n_params)
width = 0.35

for ax, m in zip(axes, metrics_list):
    ax.bar(x - width/2, Si_sobol[m]['S1'], width, label='$S_1$',
           color='C0', edgecolor='k', linewidth=0.5)
    ax.bar(x + width/2, Si_sobol[m]['ST'], width, label='$S_T$',
           color='C1', edgecolor='k', linewidth=0.5)
    ax.set_xticks(x)
    ax.set_xticklabels(param_names, rotation=45)
    ax.set_ylabel('Sobol Index')
    ax.set_title(f'Sobol: {metric_labels[m]}')
    ax.legend()

plt.suptitle('Sobol First-order ($S_1$) and Total-order ($S_T$) Indices', fontsize=14, y=1.02)
plt.tight_layout()
plt.show()


fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for ax, m in zip(axes, metrics_list):
    S2_raw = Si_sobol[m]['S2'].copy()
    S1 = Si_sobol[m]['S1']

    # SALib S2 is upper-triangular with NaN elsewhere — zero out NaN before mirroring
    S2_clean = np.where(np.isnan(S2_raw), 0.0, S2_raw)
    S2_full = S2_clean + S2_clean.T  # symmetric off-diagonal

    # Place S1 on the diagonal
    np.fill_diagonal(S2_full, S1)

    # Diverging colormap centered at 0
    vmax = np.nanmax(np.abs(S2_full))
    im = ax.imshow(S2_full, cmap='RdBu_r', aspect='equal',
                   vmin=-vmax, vmax=vmax)

    # Annotate cells with values
    for i in range(n_params):
        for j in range(n_params):
            val = S2_full[i, j]
            color = 'white' if abs(val) > 0.6 * vmax else 'black'
            fontweight = 'bold' if i == j else 'normal'
            ax.text(j, i, f'{val:.3f}', ha='center', va='center',
                    fontsize=6, color=color, fontweight=fontweight)

    ax.set_xticks(range(n_params))
    ax.set_xticklabels(param_names, rotation=45, ha='right')
    ax.set_yticks(range(n_params))
    ax.set_yticklabels(param_names)
    ax.set_title(f'{metric_labels[m]}')
    plt.colorbar(im, ax=ax, shrink=0.8)

plt.suptitle(r'Sobol Indices: $S_1$ (diagonal) and $S_{ij}$ (off-diagonal)', fontsize=14, y=1.02)
plt.tight_layout()
plt.show()





# Generate LHS ensemble
N_SAMPLES = 1000
sampler = qmc.LatinHypercube(d=n_params, seed=42)
lhs_unit = sampler.random(n=N_SAMPLES)
lhs_samples = qmc.scale(lhs_unit, lower_bounds, upper_bounds)

# Run ensemble
print(f"Running {N_SAMPLES} LHS simulations...")
Q_ensemble = np.zeros((N_SAMPLES, len(forcing)))
for i in range(N_SAMPLES):
    Q_ensemble[i] = hbv_run_ts(lhs_samples[i])
    if (i + 1) % 200 == 0:
        print(f"  Completed {i+1}/{N_SAMPLES}")

print(f"Ensemble shape: {Q_ensemble.shape}")


from scipy.spatial.distance import pdist, squareform
from pyDGSA.cluster import KMedoids
from pyDGSA.dgsa import dgsa, dgsa_interactions
from pyDGSA.plot import vert_pareto_plot, plot_cdf

# Use the post-warmup time series as multivariate response
Q_post = Q_ensemble[:, WARMUP:]
time_post = forcing["Time"].values[WARMUP:]

# Compute pairwise distances between response time series
distances = squareform(pdist(Q_post, metric="euclidean"))

# Cluster responses
n_clusters = 3
clusterer = KMedoids(n_clusters=n_clusters, max_iter=3000, tol=1e-4)
labels, medoids = clusterer.fit_predict(distances)

cluster_names = ["Low Q", "Medium Q", "High Q"]
# Sort cluster names by mean discharge of each cluster
cluster_means = [np.mean(Q_post[labels == k]) for k in range(n_clusters)]
order = np.argsort(cluster_means)
label_map = {old: new for new, old in enumerate(order)}
labels_sorted = np.array([label_map[l] for l in labels])
medoids_sorted = [medoids[order[k]] for k in range(n_clusters)]

print(f"Cluster sizes: {[np.sum(labels_sorted == k) for k in range(n_clusters)]}")
for k in range(n_clusters):
    print(f"  {cluster_names[k]}: mean Q = {np.mean(Q_post[labels_sorted == k]):.2f} m3/s")


cluster_colors = ["#4393c3", "#f4a582", "#d6604d"]

fig, axes = plt.subplots(2, 1, figsize=(14, 7))

# Full time series
ax = axes[0]
for k in range(n_clusters):
    mask = labels_sorted == k
    mu = Q_post[mask].mean(axis=0)
    ax.plot(time_post, mu, color=cluster_colors[k], lw=1.5,
            label=f'{cluster_names[k]} (n={mask.sum()})')
ax.set_ylabel('Q (m$^3$/s)')
ax.set_title('K-Medoids Clusters: Mean Discharge Time Series')
ax.legend(loc='upper right')

# Zoom into water year 1995 (Oct 1994 – Sep 1995)
ax = axes[1]
wy_mask = (time_post >= np.datetime64('1994-10-01')) & (time_post < np.datetime64('1995-10-01'))
for k in range(n_clusters):
    mask = labels_sorted == k
    mu = Q_post[mask].mean(axis=0)
    ax.plot(time_post[wy_mask], mu[wy_mask], color=cluster_colors[k], lw=2,
            label=f'{cluster_names[k]}')
ax.set_xlabel('Time')
ax.set_ylabel('Q (m$^3$/s)')
ax.set_title('Water Year 1995 (Oct 1994 – Sep 1995)')
ax.legend(loc='upper right')

plt.tight_layout()
plt.show()


# Main effects
print("Computing DGSA main effects...")
dgsa_main = dgsa(
    lhs_samples, labels_sorted,
    parameter_names=param_names, n_boots=3000, confidence=True
)
print("\nDGSA Main Effect Sensitivities:")
print(dgsa_main)

fig, ax = vert_pareto_plot(dgsa_main, confidence=True)
ax.set_title('DGSA Main Effects (full time series response)')
plt.tight_layout()
plt.show()


# Per-cluster sensitivity
dgsa_cluster = dgsa(
    lhs_samples, labels_sorted,
    parameter_names=param_names, n_boots=3000,
    output="cluster_avg", cluster_names=cluster_names
)

fig, ax = vert_pareto_plot(dgsa_cluster, fmt="cluster_avg", colors=cluster_colors)
ax.set_title('DGSA Sensitivity by Response Cluster')
plt.tight_layout()
plt.show()


# CDF plots for the most sensitive parameters
top_params = dgsa_main.sort_values('sensitivity', ascending=False).index[:4].tolist()

fig, axes = plt.subplots(1, len(top_params), figsize=(4*len(top_params), 4))
for ax, p in zip(axes, top_params):
    p_idx = param_names.index(p)
    for k in range(n_clusters):
        vals = np.sort(lhs_samples[labels_sorted == k, p_idx])
        cdf = np.arange(1, len(vals)+1) / len(vals)
        ax.plot(vals, cdf, color=cluster_colors[k], lw=2, label=cluster_names[k])
    # Prior
    vals_all = np.sort(lhs_samples[:, p_idx])
    cdf_all = np.arange(1, len(vals_all)+1) / len(vals_all)
    ax.plot(vals_all, cdf_all, 'k--', lw=1.5, label='Prior')
    ax.set_xlabel(p)
    ax.set_ylabel('CDF')
    ax.set_title(f'{p} (sens={dgsa_main.loc[p, "sensitivity"]:.2f})')
    ax.legend(fontsize=8)

plt.suptitle('Class-Conditional CDFs for Most Sensitive Parameters', fontsize=13, y=1.03)
plt.tight_layout()
plt.show()





print("Computing DGSA interaction effects (this may take a few minutes)...")
dgsa_interact = dgsa_interactions(
    lhs_samples, labels_sorted,
    parameter_names=param_names, n_bins=3, n_boots=1000
)

print("\nTop 15 Interaction Sensitivities:")
print(dgsa_interact.head(15))

fig, ax = vert_pareto_plot(dgsa_interact, np_plot="+8")
ax.set_title('DGSA Interaction Effects')
plt.tight_layout()
plt.show()


from pyDGSA.plot import interaction_matrix

fig, ax = interaction_matrix(dgsa_interact)
ax.set_title('DGSA Interaction Matrix', fontsize=13)
plt.tight_layout()
plt.show()





fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)

for ax, m in zip(axes, metrics_list):
    comp = pd.DataFrame({
        'OAT': np.abs(oat_elasticities[m]),
        'Morris': Si_morris[m]['mu_star'],
        'Sobol ST': Si_sobol[m]['ST'],
    }, index=param_names)
    comp_norm = comp / comp.max()

    x = np.arange(n_params)
    width = 0.25
    for j, (method, color) in enumerate(zip(comp_norm.columns, ['C0', 'C1', 'C2'])):
        ax.bar(x + j*width - width, comp_norm[method], width,
               label=method, color=color, edgecolor='k', linewidth=0.5)
    ax.set_xticks(x)
    ax.set_xticklabels(param_names, rotation=45)
    ax.set_title(f'{metric_labels[m]}')
    ax.set_ylabel('Normalized Sensitivity')
    ax.legend(fontsize=9)

plt.suptitle('Comparison: OAT vs Morris vs Sobol (by metric)', fontsize=14, y=1.02)
plt.tight_layout()
plt.show()


# Overall comparison including DGSA (using mean_Q for OAT/Morris/Sobol)
comparison = pd.DataFrame({
    'OAT': np.abs(oat_elasticities['mean_Q']),
    'Morris': Si_morris['mean_Q']['mu_star'],
    'Sobol ST': Si_sobol['mean_Q']['ST'],
    'DGSA': dgsa_main['sensitivity'].reindex(param_names).values,
}, index=param_names)

# Rankings
rankings = comparison.rank(ascending=False).astype(int)
print("Parameter Rankings (1 = most sensitive):")
print(rankings)

# Normalized grouped bar chart
comp_norm = comparison / comparison.max()
fig, ax = plt.subplots(figsize=(12, 6))
x = np.arange(n_params)
width = 0.2
for j, (method, color) in enumerate(zip(comp_norm.columns, ['C0', 'C1', 'C2', 'C3'])):
    ax.bar(x + j*width - 1.5*width, comp_norm[method], width,
           label=method, color=color, edgecolor='k', linewidth=0.5)
ax.set_xticks(x)
ax.set_xticklabels(param_names, rotation=45)
ax.set_ylabel('Normalized Sensitivity (max = 1)')
ax.set_title('Comparison of All Sensitivity Analysis Methods (Mean Q)')
ax.legend()
plt.tight_layout()
plt.show()



